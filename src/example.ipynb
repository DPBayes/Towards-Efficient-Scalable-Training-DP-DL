{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook\n",
    "\n",
    "In this notebook we show how to use the library to implement computionally-efficient DP-SGD with JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup (skip until 1. if you don't need the details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "import jax\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from data import load_from_huggingface\n",
    "from models import create_train_state\n",
    "\n",
    "from dp_accounting_utils import compute_epsilon, calculate_noise\n",
    "\n",
    "from jax_mask_efficient import (\n",
    "    compute_per_example_gradients_physical_batch,\n",
    "    add_trees,\n",
    "    clip_and_accumulate_physical_batch,\n",
    "    get_padded_logical_batch,\n",
    "    model_evaluation,\n",
    "    add_Gaussian_noise,\n",
    "    poisson_sample_logical_batch_size,\n",
    "    setup_physical_batches,\n",
    "    update_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Enviroment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".90\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "jax.clear_caches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Use GPU or CPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Arguments\n",
    "\n",
    "Here you can change the value of the arguments by changing the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.0005, num_steps=3, logical_bs=10, clipping_norm=0.1, target_epsilon=8, target_delta=1e-05, physical_bs=2, accountant='pld', seed=1234)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\", default=0.01, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--num_steps\", default=3, type=int, help=\"Number of steps\")\n",
    "parser.add_argument(\"--logical_bs\", default=10, type=int, help=\"Logical batch size\")\n",
    "parser.add_argument(\"--clipping_norm\", default=1, type=float, help=\"max grad norm\")\n",
    "\n",
    "parser.add_argument(\"--target_epsilon\", default=8, type=float, help=\"target epsilon\")\n",
    "parser.add_argument(\"--target_delta\", default=1e-5, type=float, help=\"target delta\")\n",
    "\n",
    "parser.add_argument(\"--physical_bs\", default=2, type=int, help=\"Physical Batch Size\")\n",
    "parser.add_argument(\"--accountant\", default=\"pld\", type=str, help=\"The privacy accountant for DP training.\")\n",
    "\n",
    "parser.add_argument(\"--seed\", default=1234, type=int)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "print(\"Used args:\", args, flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up dataset, model and DP accounting\n",
    "We show you how to setup the dataset, model and how the DP accounting works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset\n",
    "\n",
    "We load the dataset from [Hugging Face](https://huggingface.co/) but the only important thing is to have the data available as arrays. Hugging Face supports this nicely but there might be other or even better ways to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, test_images, test_labels = load_from_huggingface(\"cifar10\", cache_dir=None)\n",
    "\n",
    "train_images = train_images[:1000]\n",
    "train_labels = train_labels[:1000]\n",
    "\n",
    "test_images = test_images[:100]\n",
    "test_labels = test_labels[:100]\n",
    "\n",
    "ORIG_IMAGE_DIMENSION, RESIZED_IMAGE_DIMENSION = 32, 224\n",
    "\n",
    "num_classes = 10\n",
    "dataset_size = len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model\n",
    "\n",
    "We create a `flax.training.train_state.TrainState` and load pre-trained weights from [Hugging Face](https://huggingface.co/) using the `create_train_state` function that we provide in `src.models.py`. In this particular example, we load the weights of a `vit-base-patch16-224` and only fine-tune the last layer (classification layer).\n",
    "\n",
    "*Note: The warnings that some weights are not used are expected as we have a different number of classes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model name google/vit-base-patch16-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FlaxViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- ('classifier', 'bias'): found shape (1000,) in the checkpoint and (10,) in the model instantiated\n",
      "- ('classifier', 'kernel'): found shape (768, 1000) in the checkpoint and (768, 10) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "optimizer_config = namedtuple(\"Config\", [\"learning_rate\"])\n",
    "optimizer_config.learning_rate = args.lr\n",
    "\n",
    "state = create_train_state(\n",
    "    model_name=\"google/vit-base-patch16-224\",\n",
    "    num_classes=num_classes,\n",
    "    image_dimension=RESIZED_IMAGE_DIMENSION,\n",
    "    optimizer_config=optimizer_config,\n",
    "    layers_to_freeze=[\"vit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 DP accounting\n",
    "\n",
    "First, we compute the `subsampling_ratio` based on the `dataset_size` and the (expected) `logical_bs`. Then we compute the required DP-SGD `noise_std` based on the `subsampling_ratio` and the `num_steps` for a particular pair of `target_epsilon` and `target_delta` using a privacy accountant. At the moment the Privacy Loss Distributions (PLDs) and RDP accounting from the google [dp_accounting](https://github.com/google/differential-privacy/tree/main/python/dp_accounting) library are supported. \n",
    "\n",
    "*Note: You can also use the accounting tooling of other libraries such as the PyTorch based [opacus](https://github.com/pytorch/opacus).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_size * args.target_delta > 1.0:\n",
    "    warnings.warn(\"Your delta might be too high.\")\n",
    "\n",
    "subsampling_ratio = 1 / math.ceil(dataset_size / args.logical_bs)\n",
    "\n",
    "noise_std = calculate_noise(\n",
    "        sample_rate=subsampling_ratio,\n",
    "        target_epsilon=args.target_epsilon,\n",
    "        target_delta=args.target_delta,\n",
    "        steps=args.num_steps,\n",
    "        accountant=args.accountant,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function to process one physical batch\n",
    "\n",
    "First we define the function that computes per example gradients (`compute_per_example_gradients_physical_batch`) and clips them (`clip_and_accumulate_physical_batch`). This function can be jit compiled and then used in the full training loop later (see 3.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def process_physical_batch(t, params):\n",
    "    (\n",
    "        state,\n",
    "        accumulated_clipped_grads,\n",
    "        logical_batch_X,\n",
    "        logical_batch_y,\n",
    "        masks,\n",
    "    ) = params\n",
    "    # slice\n",
    "    start_idx = t * args.physical_bs\n",
    "    pb = jax.lax.dynamic_slice(\n",
    "        logical_batch_X,\n",
    "        (start_idx, 0, 0, 0, 0),\n",
    "        (args.physical_bs, 1, 3, ORIG_IMAGE_DIMENSION, ORIG_IMAGE_DIMENSION),\n",
    "    )\n",
    "    yb = jax.lax.dynamic_slice(logical_batch_y, (start_idx,), (args.physical_bs,))\n",
    "    mask = jax.lax.dynamic_slice(masks, (start_idx,), (args.physical_bs,))\n",
    "\n",
    "    # compute grads and clip\n",
    "    per_example_gradients = compute_per_example_gradients_physical_batch(state, pb, yb, num_classes)\n",
    "    sum_of_clipped_grads_from_pb = clip_and_accumulate_physical_batch(\n",
    "        per_example_gradients, mask, args.clipping_norm\n",
    "    )\n",
    "    accumulated_clipped_grads = add_trees(accumulated_clipped_grads, sum_of_clipped_grads_from_pb)\n",
    "\n",
    "    return (\n",
    "        state,\n",
    "        accumulated_clipped_grads,\n",
    "        logical_batch_X,\n",
    "        logical_batch_y,\n",
    "        masks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Full training loop\n",
    "\n",
    "The below cell executes the main training loop. It consists of the following parts at every step:\n",
    "\n",
    "- Poission sampling of the logical batch size (`poisson_sample_logical_batch_size`): This gives us the logical batch size using Poisson subsampling.\n",
    "- Rounding up of the logical batch size so that there are full physical batches (`setup_physical_batches`): This rounds up the logical batch size so that it is divisible in full physical batches\n",
    "- Padding of the logical batches (`get_padded_logical_batch`): Here we load the actual images and labels.\n",
    "- Computation of the per sample gradients (`jax.lax.fori_loop` using the previously defined `process_physical_batch`): This efficiently computes the per-example gradients of the logical batch.\n",
    "- Addition of noise (`add_Gaussian_noise`): Add the required noise to the accumulated gradients of the logical batch. \n",
    "\n",
    "At the end of a step the following things are executed:\n",
    "- Computation of the throughput: Compute the number of processed examples divided by the time spent.\n",
    "- Evaluate the model (`model_evaluation`): Compute the test accuracy.\n",
    "- Compute the spent privacy budget (`compute_epsilon`): Compute the spent privacy budget using a privacy accountant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Starting gradient accumulation #####\n",
      "throughput at iteration 0: 0.16923587024211884\n",
      "accuracy at iteration 0: 0.10000000149011612\n",
      "{'accountant': 'pld', 'epsilon': 7.023617560896386, 'delta': 9.999999999661252e-06}\n",
      "##### Starting gradient accumulation #####\n",
      "throughput at iteration 1: 0.1909201294183731\n",
      "accuracy at iteration 1: 0.07999999821186066\n",
      "{'accountant': 'pld', 'epsilon': 7.628718609519082, 'delta': 9.99999999951397e-06}\n",
      "##### Starting gradient accumulation #####\n",
      "throughput at iteration 2: 0.1960252970457077\n",
      "accuracy at iteration 2: 0.09000000357627869\n",
      "{'accountant': 'pld', 'epsilon': 7.999954732656329, 'delta': 9.999999999472376e-06}\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "logical_batch_sizes = []\n",
    "\n",
    "for t in range(args.num_steps):\n",
    "    sampling_rng = jax.random.key(t + 1)\n",
    "    batch_rng, binomial_rng, noise_rng = jax.random.split(sampling_rng, 3)\n",
    "\n",
    "    #######\n",
    "    # poisson subsample\n",
    "    actual_batch_size = poisson_sample_logical_batch_size(\n",
    "        binomial_rng=binomial_rng, dataset_size=dataset_size, q=subsampling_ratio\n",
    "    )\n",
    "\n",
    "    # determine padded_logical_bs so that there are full physical batches\n",
    "    # and create appropriate masks to mask out unnessary elements later\n",
    "    masks, n_physical_batches = setup_physical_batches(\n",
    "        actual_logical_batch_size=actual_batch_size,\n",
    "        physical_bs=args.physical_bs,\n",
    "    )\n",
    "\n",
    "    # get random padded logical batches that are slighly larger actual batch size\n",
    "    padded_logical_batch_X, padded_logical_batch_y = get_padded_logical_batch(\n",
    "        batch_rng=batch_rng,\n",
    "        padded_logical_batch_size=len(masks),\n",
    "        train_X=train_images,\n",
    "        train_y=train_labels,\n",
    "    )\n",
    "\n",
    "    padded_logical_batch_X = padded_logical_batch_X.reshape(-1, 1, 3, ORIG_IMAGE_DIMENSION, ORIG_IMAGE_DIMENSION)\n",
    "\n",
    "    # cast to GPU\n",
    "    if USE_GPU:\n",
    "        padded_logical_batch_X = jax.device_put(padded_logical_batch_X, jax.devices(\"gpu\")[0])\n",
    "        padded_logical_batch_y = jax.device_put(padded_logical_batch_y, jax.devices(\"gpu\")[0])\n",
    "        masks = jax.device_put(masks, jax.devices(\"gpu\")[0])\n",
    "\n",
    "    print(\"##### Starting gradient accumulation #####\", flush=True)\n",
    "    ### gradient accumulation\n",
    "    params = state.params\n",
    "\n",
    "    accumulated_clipped_grads0 = jax.tree.map(lambda x: 0.0 * x, params)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Main loop\n",
    "    _, accumulated_clipped_grads, *_ = jax.lax.fori_loop(\n",
    "        0,\n",
    "        n_physical_batches,\n",
    "        process_physical_batch,\n",
    "        (\n",
    "            state,\n",
    "            accumulated_clipped_grads0,\n",
    "            padded_logical_batch_X,\n",
    "            padded_logical_batch_y,\n",
    "            masks,\n",
    "        ),\n",
    "    )\n",
    "    noisy_grad = add_Gaussian_noise(noise_rng, accumulated_clipped_grads, noise_std, args.clipping_norm)\n",
    "\n",
    "    # update\n",
    "    state = jax.block_until_ready(update_model(state, noisy_grad))\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "\n",
    "    times.append(duration)\n",
    "    logical_batch_sizes.append(actual_batch_size)\n",
    "\n",
    "    print(f\"throughput at iteration {t}: {actual_batch_size / duration}\", flush=True)\n",
    "\n",
    "    acc_iter = model_evaluation(state, test_images, test_labels, batch_size=10, orig_image_dimension=ORIG_IMAGE_DIMENSION, use_gpu=USE_GPU)\n",
    "    print(f\"accuracy at iteration {t}: {acc_iter}\", flush=True)\n",
    "\n",
    "    # Compute privacy guarantees\n",
    "    epsilon, delta = compute_epsilon(\n",
    "        noise_multiplier=noise_std,\n",
    "        sample_rate=subsampling_ratio,\n",
    "        steps=t + 1,\n",
    "        target_delta=args.target_delta,\n",
    "        accountant=args.accountant,\n",
    "    )\n",
    "    privacy_results = {\"accountant\": args.accountant, \"epsilon\": epsilon, \"delta\": delta}\n",
    "    print(privacy_results, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Final Model evaluation\n",
    "Here we computate of the throughput (num processed examples/time spent) and final test accuracy (`model_evaluation`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times \n",
      " [53.18021559715271, 36.66454553604126, 40.81105709075928]\n",
      "batch sizes \n",
      "  [Array(9, dtype=int32), Array(7, dtype=int32), Array(8, dtype=int32)]\n",
      "accuracy at end of training 0.09\n"
     ]
    }
   ],
   "source": [
    "acc_last = model_evaluation(state, test_images, test_labels, batch_size=10, use_gpu=USE_GPU, orig_image_dimension=ORIG_IMAGE_DIMENSION)\n",
    "\n",
    "print(\"times \\n\", times, flush=True)\n",
    "\n",
    "print(\"batch sizes \\n \", logical_batch_sizes, flush=True)\n",
    "\n",
    "print(\"accuracy at end of training\", acc_last, flush=True)\n",
    "thr = np.mean(np.array(logical_batch_sizes) / np.array(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
